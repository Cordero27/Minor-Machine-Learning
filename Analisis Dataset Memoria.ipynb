{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSWvJDPbuTN0LtKbBjCwAT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cordero27/Minor-Machine-Learning/blob/main/Analisis%20Dataset%20Memoria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de conjunto de datos preliminar para Memoria"
      ],
      "metadata": {
        "id": "wkfjGndxygIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El Conjunto de datos a estudiar pertenece a la captura de datos de una colmena de abejas durante un año, que se realizó en la ciudad de Würzburg en 2017. Los parámetros trabajados durante el proyecto son flujo de abejas que ingresan y egresan de la colmena, humedad y temperatura interior de la colmena, y peso de esta. Las unidades de medida de estos últimos tres parámetros son porcentaje, Celcius y Kilogramos respectivamente.\n",
        "\n",
        "### Cabe destacar, que solo se usan los archivos que tienen el año 2017 en su nombre, pues los demas corresponden a un estudio de 2017-2019 que tienen un periodo no medido en el año 2018.\n",
        "\n",
        "## Para procesar este conjunto de datos se usa OSEMN."
      ],
      "metadata": {
        "id": "CAfIQn31vNpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Entrega 01 - OBTAIN"
      ],
      "metadata": {
        "id": "3EmT_TFWBmG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#Librerias a utilizar\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Fuente del dataset: https://www.kaggle.com/datasets/se18m502/bee-hive-metrics\n",
        "\n",
        "flow = pd.read_csv('https://raw.githubusercontent.com/Cordero27/Minor-Machine-Learning/main/DatasetPreparado/flow_2017.csv', on_bad_lines='skip')\n",
        "humidity = pd.read_csv('https://raw.githubusercontent.com/Cordero27/Minor-Machine-Learning/main/DatasetPreparado/humidity_2017.csv', on_bad_lines='skip')\n",
        "temperature = pd.read_csv('https://raw.githubusercontent.com/Cordero27/Minor-Machine-Learning/main/DatasetPreparado/temperature_2017.csv', on_bad_lines='skip')\n",
        "weight = pd.read_csv('https://raw.githubusercontent.com/Cordero27/Minor-Machine-Learning/main/DatasetPreparado/weight_2017.csv', on_bad_lines='skip')\n",
        "\n",
        "#Observamos el contenido adquirido del archivo del flujo\n",
        "flow.head()"
      ],
      "metadata": {
        "id": "R04-XK98wKGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observamos el contenido adquirido del archivo de la humedad\n",
        "humidity.head()"
      ],
      "metadata": {
        "id": "voA1zkYB-MTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observamos el contenido adquirido del archivo de la temperatura\n",
        "temperature.head()"
      ],
      "metadata": {
        "id": "UlAtLNmz-Sed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Observamos el contenido adquirido del archivo del peso\n",
        "weight.head()"
      ],
      "metadata": {
        "id": "g7rUXYoe-SwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Entrega 02 - SCRUB"
      ],
      "metadata": {
        "id": "UrSIZErtBxuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comenzamos verificando si exiten valores NAs en los conjuntos de datos"
      ],
      "metadata": {
        "id": "GoXgC4pEB25M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos si existen NAs dentro del conjunto de datos.\n",
        "flow.info()"
      ],
      "metadata": {
        "id": "f65QXTUfAl0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos si existen NAs dentro del conjunto de datos.\n",
        "humidity.info()"
      ],
      "metadata": {
        "id": "ZzEuqT9xBwT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos si existen NAs dentro del conjunto de datos.\n",
        "temperature.info()"
      ],
      "metadata": {
        "id": "i30s2nT7Ce54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificamos si existen NAs dentro del conjunto de datos.\n",
        "weight.info()"
      ],
      "metadata": {
        "id": "5qRekOnrCigG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como observamos, solo se detectan NAs en el archivo de la temperatura, pues en este es el único donde varía la cantidad de filas que contiene con la cantidad de elementos not-null encontrados. Pero como solo son 3 elementos, revisamos el archivo para en lugar de eliminarlas, podemos insertar un valor consistente a los cercanos"
      ],
      "metadata": {
        "id": "qJKH_fnEDc_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Es necesario tener un detalle de cuantas variables están considerando y el tipo de datos que pertenecen para verificar si es posible utilizarla directamente para el modelo predictivo o si es necesario realizar una transformación del tipo de dato.*"
      ],
      "metadata": {
        "id": "5C6bszikvwBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(flow.info())\n",
        "print(\"_________________________________________\")\n",
        "print(humidity.info())\n",
        "print(\"_________________________________________\")\n",
        "print(temperature.info())\n",
        "print(\"_________________________________________\")\n",
        "print(weight.info())"
      ],
      "metadata": {
        "id": "DWjGmlxkpYMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Muchas de las variables contenidas en el DataFrame originalmente se encuentran en el tipo de dato 'object'. En ese formato las variables no pueden ser interpretadas como vectores. Recordemos que la mayoría de los modelos de Machine Learning estudiados operan en el espacio vectorial. Por esta razón las variables que formarán parte del modelo predictivo deben ser transformadas a un formato numérico como 'Float' o 'integer'.*"
      ],
      "metadata": {
        "id": "n_96jl3ov21g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finalmente, ya podemos cambiar el tipado de la columna, y aprovechamos de pasar la otra columna tipo object (activity) por category\n",
        "#flow.loc[:, \"flow\"] = flow.astype({'flow': 'int16'})\n",
        "humidity.loc[:, \"humidity\"] = humidity.astype({'humidity': 'float16'})\n",
        "temperature.loc[:, \"temperature\"] = temperature.astype({'temperature': 'float16'})\n",
        "weight.loc[:, \"weight\"] = weight.astype({'weight': 'float16'})\n",
        "print(flow.info())\n",
        "print(\"_________________________________________\")\n",
        "print(humidity.info())\n",
        "print(\"_________________________________________\")\n",
        "print(temperature.info())\n",
        "print(\"_________________________________________\")\n",
        "print(weight.info())"
      ],
      "metadata": {
        "id": "Hlrc2AdkvlQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrega 03 - EXPLORE"
      ],
      "metadata": {
        "id": "iApjGUSd1BMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenemos las estadisticas correspondientes a cada columna\n",
        "flow.describe()"
      ],
      "metadata": {
        "id": "hTOsQCzVvlop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "humidity.describe()"
      ],
      "metadata": {
        "id": "5KqdLilxvl-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temperature.describe()"
      ],
      "metadata": {
        "id": "uGSJmgJu1KqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este dataset tiene una perdida del peso el dia 18 de Septiembre de 10:20 a 12:30."
      ],
      "metadata": {
        "id": "QyiMfwuWSGb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight.describe()"
      ],
      "metadata": {
        "id": "0CpTfHLj1K9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plt.plot(flow.timestamp, flow['flow'])\n",
        "#plt.title('Comportamiento flow', fontsize=15)\n",
        "#plt.xlabel('Fecha')\n",
        "#plt.ylabel('Cantidad abejas')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "5atOzCzw6aoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(humidity.timestamp, humidity['humidity'])\n",
        "plt.title('Comportamiento humedad', fontsize=15)\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Porcentaje')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R_XD61ZU1myb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(temperature.timestamp, temperature['temperature'])\n",
        "plt.title('Comportamiento temperatura', fontsize=15, color='red')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Grados Celcius')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vroTHHOt3Yup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(weight.timestamp, weight['weight'])\n",
        "plt.title('Comportamiento peso', fontsize=15, color='black')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Kilogramos')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JCnPNnWZ3uIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tempHumWei = pd.concat([flow['flow'], temperature['temperature'], humidity['humidity'], weight['weight']], axis=1, join='inner').sort_index()\n",
        "corr_mat = tempHumWei.corr(method='pearson')\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')"
      ],
      "metadata": {
        "id": "5W6lNIVkOzur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Para el caso de la estandarización de los datos se pueden usar distintas técnicas, como la normalización 01, estandarización entre -1 y 1 o el denominado Z-Score. En este caso usaremos la función Standard Scaler de Python con sus valores por defecto.*"
      ],
      "metadata": {
        "id": "ZoO-8L5YE7JJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss= StandardScaler()\n",
        "\n",
        "flow['flow'] = ss.fit_transform(flow[['flow']])\n",
        "humidity['humidity'] = ss.fit_transform(humidity[['humidity']])\n",
        "temperature['temperature'] = ss.fit_transform(temperature[['temperature']])\n",
        "weight['weight'] = ss.fit_transform(weight[['weight']])"
      ],
      "metadata": {
        "id": "nm2onezg7BsJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}